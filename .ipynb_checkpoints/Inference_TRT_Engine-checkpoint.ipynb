{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/seyedalicheraghi/Image-Classification/blob/master/Inference_TRT_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_ZBrWY0FAaP"
   },
   "source": [
    "Next step after producing the TRT Engine is to run it on an image to check its result. This session provide the code to achieve this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eQgbUOKIHMi"
   },
   "source": [
    "### Install requried libraries in Google Colab\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrCEp0YwHJcS",
    "outputId": "9d650a10-5a02-4bfe-8e15-b67b055fcad7"
   },
   "outputs": [],
   "source": [
    "!pip install pycuda # install cuda\n",
    "!pip install tensorrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9i0KY7s4IXqM"
   },
   "source": [
    "#### Check TensorRT version\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "15MnskZTIaVl",
    "outputId": "dc5be8ca-45b7-4ec1-9e09-816ecb79b344"
   },
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "trt.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3OINNECIksU"
   },
   "source": [
    "#### Import requried libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EYH-wi_HDJt"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import tensorrt as trt\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qigYtiWGJigE"
   },
   "source": [
    "#### Define constants\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffW0wyjSJh5z"
   },
   "outputs": [],
   "source": [
    "engine_path = 'model.trt'\n",
    "input_name = 'conv2d_input'\n",
    "Input_shape = (3,256,256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI28CRzjGL5W"
   },
   "source": [
    "**Step 1:** Load the engine we created in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eOsp1s4kGLRs",
    "outputId": "84ebaf92-e4c0-4102-d61d-a18bfe28ead2"
   },
   "outputs": [],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "print(\"Load the created tensorrt engine\")\n",
    "with open(engine_path, 'rb') as f:\n",
    "    engine_data = f.read()\n",
    "engine = trt_runtime.deserialize_cuda_engine(engine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPjdVn5CJA-r"
   },
   "outputs": [],
   "source": [
    "# Simple helper data class that's a little nicer to use than a 2-tuple.\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRJmEZlFE4_l"
   },
   "outputs": [],
   "source": [
    "# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\n",
    "def allocate_buffers(engine):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    stream = cuda.Stream()\n",
    "    for binding in engine:\n",
    "        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
    "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "        # Allocate host and device buffers\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        # Append the device buffer to device bindings.\n",
    "        bindings.append(int(device_mem))\n",
    "        # Append to the appropriate list.\n",
    "        if engine.binding_is_input(binding):\n",
    "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        else:\n",
    "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "    return inputs, outputs, bindings, stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZQdLD4SHbsT",
    "outputId": "da9615cb-ecbd-4a5c-d033-43c38e746f1d"
   },
   "outputs": [],
   "source": [
    "inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVvg_s2sH2h8"
   },
   "outputs": [],
   "source": [
    "test_image = np.float32(np.random.rand(Input_shape[0] ,Input_shape[1], Input_shape[2]))\n",
    "\n",
    "inputs[0].host = test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3S-kIJ-pJySx"
   },
   "outputs": [],
   "source": [
    "def do_inference_v2(context, bindings, inputs, outputs, stream):\n",
    "    # Transfer input data to the GPU.\n",
    "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
    "    # # Run inference.\n",
    "    # context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
    "    # # Transfer predictions back from the GPU.\n",
    "    # [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
    "    # # Synchronize the stream\n",
    "    # stream.synchronize()\n",
    "    # # Return only the host outputs.\n",
    "    # return [out.host for out in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "4GN-6FrZJM7b",
    "outputId": "d33907b1-5ee8-4a45-9d00-81dd6563c068"
   },
   "outputs": [],
   "source": [
    "trt_outputs = do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAr7Hf6tJzRI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO66/gL70tZMWMuzRU1XFWB",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
