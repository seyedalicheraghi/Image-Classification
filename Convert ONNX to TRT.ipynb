{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca78838-49e2-48f3-a392-c18b5041cce3",
   "metadata": {},
   "source": [
    "# TensorRT\n",
    "\n",
    "NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a81af-86a8-46cf-b49f-4e1665689843",
   "metadata": {},
   "source": [
    "This session shares required codes to how you can take an existing ONNX model built with a deep learning framework and build a TensorRT engine using the provided parsers. The Developer Guide also provides step-by-step instructions for common user tasks such as creating a TensorRT network definition, invoking the TensorRT builder, serializing and deserializing, and how to feed the engine with data and perform inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd3fc2-6033-4abf-8289-ec743debdaca",
   "metadata": {},
   "source": [
    "Since runnning these code requires NVIDIA GPU, we used Google Colab. Please keep in mind (based on my experience) generating TensorRT worked based on which NVIDIA GPU you use. It means you cannot generate a TensorRT Engine on one GPU and use it in another GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed1406-cf28-47f9-8ed6-7494150ecfdb",
   "metadata": {},
   "source": [
    "### Install requried libraries in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be45415-e5d5-4692-87c6-0b4ec9cba1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycuda # install cuda\n",
    "!pip install tensorrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b27fe81-a246-47af-92bc-ddcaf74fcfd2",
   "metadata": {},
   "source": [
    "#### Check TensorRT version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16afbbcd-8763-4765-94f4-1bc9f93a6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "trt.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa3167-f839-4250-835e-be95081f4395",
   "metadata": {},
   "source": [
    "#### Import requried libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a3232-33d2-490b-94f8-481f46036edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import os\n",
    "import argparse\n",
    "import tensorrt as trt\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81218d7b-c361-406e-82f8-0f3fa752c754",
   "metadata": {},
   "source": [
    "#### Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b90e2f7-b55d-423f-947f-f9d353877bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE) if verbose else trt.Logger()\n",
    "MAX_BATCH_SIZE = 1\n",
    "input_name = 'input.1'\n",
    "Input_shape = (3,608,608)\n",
    "model_name = \"model\"\n",
    "fp16 = False\n",
    "int8 = False\n",
    "dla_core = -1 \n",
    "verbose = False\n",
    "engine_path = '%s.trt' % model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc12df5-1cd7-4a0f-8e8b-c1b3a661b0e6",
   "metadata": {},
   "source": [
    "#### Load created ONNX model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956023b2-d95a-4240-8814-be55d1ac43e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_onnx(model_name):\n",
    "    \"\"\"Read the ONNX file.\"\"\"\n",
    "    onnx_path = '%s.onnx' % model_name\n",
    "    if not os.path.isfile(onnx_path):\n",
    "        print('ERROR: file (%s) not found!  You might want to run yolo_to_onnx.py first to generate it.' % onnx_path)\n",
    "        return None\n",
    "    else:\n",
    "        with open(onnx_path, 'rb') as f:\n",
    "            return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b939533-8e89-4af0-ab34-7274317c922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_data = load_onnx(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9846338-8e2e-473f-8b9c-f2578bb046c9",
   "metadata": {},
   "source": [
    "#### Set network input batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4f1b5-4d6c-454e-ae07-d1bf0c272db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_net_batch(network, batch_size):\n",
    "    \"\"\"Set network input batch size.\n",
    "\n",
    "    The ONNX file might have been generated with a different batch size,\n",
    "    say, 64.\n",
    "    \"\"\"\n",
    "    if trt.__version__[0] >= '7':\n",
    "        shape = list(network.get_input(0).shape)\n",
    "        shape[0] = batch_size\n",
    "        network.get_input(0).shape = shape\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3608718f-9846-4b7e-8ce2-44ee9395fd49",
   "metadata": {},
   "source": [
    "#### Build a TensorRT engine from ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d446bf7-e5ef-4a4b-bcfa-4d5f3cab0ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_engine(model_name,do_fp16, do_int8, dla_core, verbose=False):\n",
    "    print('Loading the ONNX file...')\n",
    "    onnx_data = load_onnx(model_name)\n",
    "    if onnx_data is None:\n",
    "        return None\n",
    "\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE) if verbose else trt.Logger()\n",
    "    EXPLICIT_BATCH = [] if trt.__version__[0] < '7' else \\\n",
    "        [1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)]\n",
    "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network(*EXPLICIT_BATCH) as network, trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "        if do_int8 and not builder.platform_has_fast_int8:\n",
    "            raise RuntimeError('INT8 not supported on this platform')\n",
    "        if not parser.parse(onnx_data):\n",
    "            print('ERROR: Failed to parse the ONNX file.')\n",
    "            for error in range(parser.num_errors):\n",
    "                print(parser.get_error(error))\n",
    "            return None\n",
    "        network = set_net_batch(network, MAX_BATCH_SIZE)\n",
    "\n",
    "\n",
    "        print('Building the TensorRT engine.  This would take a while...')\n",
    "        print('(Use \"--verbose\" or \"-v\" to enable verbose logging.)')\n",
    "        # new API: build_engine() with builder config\n",
    "        builder.max_batch_size = MAX_BATCH_SIZE\n",
    "        config = builder.create_builder_config()\n",
    "        config.max_workspace_size = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "        config.set_flag(trt.BuilderFlag.GPU_FALLBACK)\n",
    "        profile = builder.create_optimization_profile()\n",
    "        profile.set_shape(input_name, (MAX_BATCH_SIZE, Input_shape[0], Input_shape[1], Input_shape[2]),\n",
    "                          (MAX_BATCH_SIZE, Input_shape[0], Input_shape[1], Input_shape[2]),(MAX_BATCH_SIZE, Input_shape[0], Input_shape[1], Input_shape[2]))\n",
    "        config.add_optimization_profile(profile)\n",
    "        engine = builder.build_engine(network, config)\n",
    "        if engine is not None:\n",
    "            print('Completed creating engine.')\n",
    "        return engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd96ebe3-9fe2-4712-ba5f-39a1b14338eb",
   "metadata": {},
   "source": [
    "#### Create the Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb8a4a-4086-48b4-8e3f-e6d1aa8d7275",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = build_engine(model_name, fp16, int8, dla_core, verbose)\n",
    "if engine is None:\n",
    "      raise SystemExit('ERROR: failed to build the TensorRT engine!')\n",
    "with open(engine_path, 'wb') as f:\n",
    "      f.write(engine.serialize())\n",
    "print('Serialized the TensorRT engine to file: %s' % engine_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
