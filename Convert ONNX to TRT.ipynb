{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fca78838-49e2-48f3-a392-c18b5041cce3",
      "metadata": {
        "id": "fca78838-49e2-48f3-a392-c18b5041cce3"
      },
      "source": [
        "# TensorRT\n",
        "\n",
        "NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88a81af-86a8-46cf-b49f-4e1665689843",
      "metadata": {
        "id": "b88a81af-86a8-46cf-b49f-4e1665689843"
      },
      "source": [
        "This session shares required codes to how you can take an existing ONNX model built with a deep learning framework and build a TensorRT engine using the provided parsers. The Developer Guide also provides step-by-step instructions for common user tasks such as creating a TensorRT network definition, invoking the TensorRT builder, serializing and deserializing, and how to feed the engine with data and perform inference."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fefd3fc2-6033-4abf-8289-ec743debdaca",
      "metadata": {
        "id": "fefd3fc2-6033-4abf-8289-ec743debdaca"
      },
      "source": [
        "Since runnning these code requires NVIDIA GPU, we used Google Colab. Please keep in mind (based on my experience) generating TensorRT worked based on which NVIDIA GPU you use. It means you cannot generate a TensorRT Engine on one GPU and use it in another GPU. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42ed1406-cf28-47f9-8ed6-7494150ecfdb",
      "metadata": {
        "id": "42ed1406-cf28-47f9-8ed6-7494150ecfdb"
      },
      "source": [
        "### Install requried libraries in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7be45415-e5d5-4692-87c6-0b4ec9cba1a5",
      "metadata": {
        "id": "7be45415-e5d5-4692-87c6-0b4ec9cba1a5",
        "outputId": "6d0866f7-e2c4-4f73-9de8-95eb36771e81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycuda\n",
            "  Downloading pycuda-2022.2.2.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 9.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from pycuda) (1.4.4)\n",
            "Collecting pytools>=2011.2\n",
            "  Downloading pytools-2022.1.13.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 11.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytools>=2011.2->pycuda) (2.6.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.8/dist-packages (from pytools>=2011.2->pycuda) (4.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from mako->pycuda) (2.0.1)\n",
            "Building wheels for collected packages: pycuda, pytools\n",
            "  Building wheel for pycuda (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2022.2.2-cp38-cp38-linux_x86_64.whl size=646530 sha256=4911b76afcbf4e690f72876e5ff0947db0d9abbcc968aa1ae3a2e0137f48574f\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/41/0d/7cecb04af969d283ebe4a69579a8b2baec0d010a1ac4159f7e\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2022.1.13-py2.py3-none-any.whl size=66024 sha256=e75dd9049a26abececd06ba67f857debd2d29e2a9f843fe029a6cc2c4186666d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/c1/bb/26ba70fb9d10f195249ef4e170a92ae83e7534e55b67786fd9\n",
            "Successfully built pycuda pytools\n",
            "Installing collected packages: pytools, mako, pycuda\n",
            "Successfully installed mako-1.2.4 pycuda-2022.2.2 pytools-2022.1.13\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorrt\n",
            "  Downloading tensorrt-8.5.2.2-cp38-none-manylinux_2_17_x86_64.whl (549.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 549.2 MB 22 kB/s \n",
            "\u001b[?25hCollecting nvidia-cublas-cu11\n",
            "  Downloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 417.9 MB 27 kB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11\n",
            "  Downloading nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 728.5 MB 5.6 kB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[K     |████████████████████████████████| 875 kB 52.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: nvidia-cublas-cu11, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, tensorrt\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 tensorrt-8.5.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pycuda # install cuda\n",
        "!pip install tensorrt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3fa3167-f839-4250-835e-be95081f4395",
      "metadata": {
        "id": "f3fa3167-f839-4250-835e-be95081f4395"
      },
      "source": [
        "#### Import requried libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f18a3232-33d2-490b-94f8-481f46036edb",
      "metadata": {
        "id": "f18a3232-33d2-490b-94f8-481f46036edb"
      },
      "outputs": [],
      "source": [
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "import os\n",
        "import argparse\n",
        "import tensorrt as trt\n",
        "import random\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pycuda.autoinit\n",
        "import pycuda.driver as cuda\n",
        "import tensorrt as trt\n",
        "import numpy as np\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81218d7b-c361-406e-82f8-0f3fa752c754",
      "metadata": {
        "id": "81218d7b-c361-406e-82f8-0f3fa752c754"
      },
      "source": [
        "#### Define Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "2b90e2f7-b55d-423f-947f-f9d353877bf8",
      "metadata": {
        "id": "2b90e2f7-b55d-423f-947f-f9d353877bf8"
      },
      "outputs": [],
      "source": [
        "verbose = False\n",
        "TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE) if verbose else trt.Logger()\n",
        "MAX_BATCH_SIZE = 1\n",
        "input_name = 'input.1'\n",
        "Input_shape = (3,256,256)\n",
        "model_name = \"model\"\n",
        "fp16 = False\n",
        "int8 = False\n",
        "dla_core = -1 \n",
        "verbose = False\n",
        "engine_path = '%s.trt' % model_name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc12df5-1cd7-4a0f-8e8b-c1b3a661b0e6",
      "metadata": {
        "id": "4bc12df5-1cd7-4a0f-8e8b-c1b3a661b0e6"
      },
      "source": [
        "#### Load created ONNX model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "956023b2-d95a-4240-8814-be55d1ac43e9",
      "metadata": {
        "id": "956023b2-d95a-4240-8814-be55d1ac43e9"
      },
      "outputs": [],
      "source": [
        "def load_onnx(model_name):\n",
        "    \"\"\"Read the ONNX file.\"\"\"\n",
        "    onnx_path = '%s.onnx' % model_name\n",
        "    if not os.path.isfile(onnx_path):\n",
        "        print('ERROR: file (%s) not found!  You might want to run yolo_to_onnx.py first to generate it.' % onnx_path)\n",
        "        return None\n",
        "    else:\n",
        "        with open(onnx_path, 'rb') as f:\n",
        "            return f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "3b939533-8e89-4af0-ab34-7274317c922c",
      "metadata": {
        "id": "3b939533-8e89-4af0-ab34-7274317c922c"
      },
      "outputs": [],
      "source": [
        "onnx_data = load_onnx(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9846338-8e2e-473f-8b9c-f2578bb046c9",
      "metadata": {
        "id": "d9846338-8e2e-473f-8b9c-f2578bb046c9"
      },
      "source": [
        "#### Set network input batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "5cb4f1b5-4d6c-454e-ae07-d1bf0c272db5",
      "metadata": {
        "id": "5cb4f1b5-4d6c-454e-ae07-d1bf0c272db5"
      },
      "outputs": [],
      "source": [
        "def set_net_batch(network, batch_size):\n",
        "    \"\"\"Set network input batch size.\n",
        "\n",
        "    The ONNX file might have been generated with a different batch size,\n",
        "    say, 64.\n",
        "    \"\"\"\n",
        "    if trt.__version__[0] >= '7':\n",
        "        shape = list(network.get_input(0).shape)\n",
        "        shape[0] = batch_size\n",
        "        network.get_input(0).shape = shape\n",
        "    return network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3608718f-9846-4b7e-8ce2-44ee9395fd49",
      "metadata": {
        "id": "3608718f-9846-4b7e-8ce2-44ee9395fd49"
      },
      "source": [
        "#### Build a TensorRT engine from ONNX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "7d446bf7-e5ef-4a4b-bcfa-4d5f3cab0ce3",
      "metadata": {
        "id": "7d446bf7-e5ef-4a4b-bcfa-4d5f3cab0ce3"
      },
      "outputs": [],
      "source": [
        "def build_engine(model_name,do_fp16, do_int8, dla_core, verbose=False):\n",
        "    print('Loading the ONNX file...')\n",
        "    onnx_data = load_onnx(model_name)\n",
        "    if onnx_data is None:\n",
        "        return None\n",
        "\n",
        "    TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE) if verbose else trt.Logger()\n",
        "    EXPLICIT_BATCH = [] if trt.__version__[0] < '7' else \\\n",
        "        [1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)]\n",
        "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network(*EXPLICIT_BATCH) as network, trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
        "        if do_int8 and not builder.platform_has_fast_int8:\n",
        "            raise RuntimeError('INT8 not supported on this platform')\n",
        "        if not parser.parse(onnx_data):\n",
        "            print('ERROR: Failed to parse the ONNX file.')\n",
        "            for error in range(parser.num_errors):\n",
        "                print(parser.get_error(error))\n",
        "            return None\n",
        "        network = set_net_batch(network, MAX_BATCH_SIZE)\n",
        "\n",
        "\n",
        "        print('Building the TensorRT engine.  This would take a while...')\n",
        "        print('(Use \"--verbose\" or \"-v\" to enable verbose logging.)')\n",
        "        # new API: build_engine() with builder config\n",
        "        builder.max_batch_size = MAX_BATCH_SIZE\n",
        "        config = builder.create_builder_config()\n",
        "        config.max_workspace_size = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "        config.set_flag(trt.BuilderFlag.GPU_FALLBACK)\n",
        "        profile = builder.create_optimization_profile()\n",
        "        profile.set_shape(input_name, (MAX_BATCH_SIZE, Input_shape[0], Input_shape[1], Input_shape[2]),\n",
        "                          (MAX_BATCH_SIZE, Input_shape[0], Input_shape[1], Input_shape[2]),(MAX_BATCH_SIZE, Input_shape[0], Input_shape[1], Input_shape[2]))\n",
        "        config.add_optimization_profile(profile)\n",
        "        engine = builder.build_engine(network, config)\n",
        "        if engine is not None:\n",
        "            print('Completed creating engine.')\n",
        "        return engine"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd96ebe3-9fe2-4712-ba5f-39a1b14338eb",
      "metadata": {
        "id": "fd96ebe3-9fe2-4712-ba5f-39a1b14338eb"
      },
      "source": [
        "#### Create the Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "4efb8a4a-4086-48b4-8e3f-e6d1aa8d7275",
      "metadata": {
        "id": "4efb8a4a-4086-48b4-8e3f-e6d1aa8d7275",
        "outputId": "0fc7a48f-e809-45e2-bc9d-6cd3c1799244",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the ONNX file...\n",
            "Building the TensorRT engine.  This would take a while...\n",
            "(Use \"--verbose\" or \"-v\" to enable verbose logging.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-1848f7112c03>:24: DeprecationWarning: Use network created with NetworkDefinitionCreationFlag::EXPLICIT_BATCH flag instead.\n",
            "  builder.max_batch_size = MAX_BATCH_SIZE\n",
            "<ipython-input-45-1848f7112c03>:26: DeprecationWarning: Use set_memory_pool_limit instead.\n",
            "  config.max_workspace_size = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
            "<ipython-input-45-1848f7112c03>:32: DeprecationWarning: Use build_serialized_network instead.\n",
            "  engine = builder.build_engine(network, config)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed creating engine.\n",
            "Serialized the TensorRT engine to file: model.trt\n"
          ]
        }
      ],
      "source": [
        "engine = build_engine(model_name, fp16, int8, dla_core, verbose)\n",
        "if engine is None:\n",
        "      raise SystemExit('ERROR: failed to build the TensorRT engine!')\n",
        "with open(engine_path, 'wb') as f:\n",
        "      f.write(engine.serialize())\n",
        "print('Serialized the TensorRT engine to file: %s' % engine_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorrt as trt\n",
        "trt.__version__"
      ],
      "metadata": {
        "id": "pewG5LEPMUra",
        "outputId": "bb2b6a43-145b-4140-f656-e9203927da3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "pewG5LEPMUra",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'8.5.2.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import argparse\n",
        "import tensorrt as trt\n",
        "import random\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pycuda.autoinit\n",
        "import pycuda.driver as cuda\n",
        "import tensorrt as trt\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "3mRu-L73OLFz"
      },
      "id": "3mRu-L73OLFz",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
        "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
        "print(\"Load the created tensorrt engine\")\n",
        "with open(engine_path, 'rb') as f:\n",
        "    engine_data = f.read()\n",
        "engine = trt_runtime.deserialize_cuda_engine(engine_data)"
      ],
      "metadata": {
        "id": "KD0wC8hSONHV",
        "outputId": "47b03db9-577c-4755-9aa8-98ebb5e56d01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KD0wC8hSONHV",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the created tensorrt engine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple helper data class that's a little nicer to use than a 2-tuple.\n",
        "class HostDeviceMem(object):\n",
        "    def __init__(self, host_mem, device_mem):\n",
        "        self.host = host_mem\n",
        "        self.device = device_mem\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__str__()"
      ],
      "metadata": {
        "id": "3DQGC4r3OSvq"
      },
      "id": "3DQGC4r3OSvq",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\n",
        "def allocate_buffers(engine):\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "    bindings = []\n",
        "    stream = cuda.Stream()\n",
        "    for binding in engine:\n",
        "        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
        "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
        "        # Allocate host and device buffers\n",
        "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
        "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
        "        # Append the device buffer to device bindings.\n",
        "        bindings.append(int(device_mem))\n",
        "        # Append to the appropriate list.\n",
        "        if engine.binding_is_input(binding):\n",
        "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "        else:\n",
        "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "    return inputs, outputs, bindings, stream"
      ],
      "metadata": {
        "id": "K4RGjLaDOU37"
      },
      "id": "K4RGjLaDOU37",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
        "context = engine.create_execution_context()"
      ],
      "metadata": {
        "id": "2Q8RMXHEOXGL",
        "outputId": "80db3698-c4ed-4b0e-dfc4-c4d0c120c5de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2Q8RMXHEOXGL",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-4e2a1eedc286>:8: DeprecationWarning: Use get_tensor_shape instead.\n",
            "  size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
            "<ipython-input-51-4e2a1eedc286>:8: DeprecationWarning: Use network created with NetworkDefinitionCreationFlag::EXPLICIT_BATCH flag instead.\n",
            "  size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
            "<ipython-input-51-4e2a1eedc286>:9: DeprecationWarning: Use get_tensor_dtype instead.\n",
            "  dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
            "<ipython-input-51-4e2a1eedc286>:16: DeprecationWarning: Use get_tensor_mode instead.\n",
            "  if engine.binding_is_input(binding):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = np.float32(np.random.rand(Input_shape[0] ,Input_shape[1], Input_shape[2]))\n",
        "\n",
        "inputs[0].host = test_image"
      ],
      "metadata": {
        "id": "G94j4OJPOZQq"
      },
      "id": "G94j4OJPOZQq",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_inference_v2(context, bindings, inputs, outputs, stream):\n",
        "    # Transfer input data to the GPU.\n",
        "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
        "    # Run inference.\n",
        "    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
        "    # Transfer predictions back from the GPU.\n",
        "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
        "    # Synchronize the stream\n",
        "    stream.synchronize()\n",
        "    # Return only the host outputs.\n",
        "    return [out.host for out in outputs]"
      ],
      "metadata": {
        "id": "2CzzFYJ1ObrJ"
      },
      "id": "2CzzFYJ1ObrJ",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trt_outputs = do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)"
      ],
      "metadata": {
        "id": "nSwYU0o6Od1u"
      },
      "id": "nSwYU0o6Od1u",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aWJ2OCCKOfvo"
      },
      "id": "aWJ2OCCKOfvo",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "tf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}