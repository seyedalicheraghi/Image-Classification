{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO66/gL70tZMWMuzRU1XFWB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyedalicheraghi/Image-Classification/blob/master/Inference_TRT_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step after producing the TRT Engine is to run it on an image to check its result. This session provide the code to achieve this goal."
      ],
      "metadata": {
        "id": "i_ZBrWY0FAaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install requried libraries in Google Colab\n",
        "---"
      ],
      "metadata": {
        "id": "9eQgbUOKIHMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda # install cuda\n",
        "!pip install tensorrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrCEp0YwHJcS",
        "outputId": "9d650a10-5a02-4bfe-8e15-b67b055fcad7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycuda\n",
            "  Downloading pycuda-2022.2.2.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 17.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from pycuda) (1.4.4)\n",
            "Collecting mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 9.6 MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "  Downloading pytools-2022.1.13.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 10.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytools>=2011.2->pycuda) (2.6.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.8/dist-packages (from pytools>=2011.2->pycuda) (4.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from mako->pycuda) (2.0.1)\n",
            "Building wheels for collected packages: pycuda, pytools\n",
            "  Building wheel for pycuda (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2022.2.2-cp38-cp38-linux_x86_64.whl size=646530 sha256=2730de5c816978d84be0eaabd7ae07de0a4d397812a97fa492362b1dd417bc0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/41/0d/7cecb04af969d283ebe4a69579a8b2baec0d010a1ac4159f7e\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2022.1.13-py2.py3-none-any.whl size=66024 sha256=3dfe6b4e18897a60891dee44b5e606500b44b30b07dbe12502e1de8088f1dfaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/c1/bb/26ba70fb9d10f195249ef4e170a92ae83e7534e55b67786fd9\n",
            "Successfully built pycuda pytools\n",
            "Installing collected packages: pytools, mako, pycuda\n",
            "Successfully installed mako-1.2.4 pycuda-2022.2.2 pytools-2022.1.13\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorrt\n",
            "  Downloading tensorrt-8.5.2.2-cp38-none-manylinux_2_17_x86_64.whl (549.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 549.2 MB 21 kB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[K     |████████████████████████████████| 875 kB 61.9 MB/s \n",
            "\u001b[?25hCollecting nvidia-cublas-cu11\n",
            "  Downloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 417.9 MB 27 kB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11\n",
            "  Downloading nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 728.5 MB 5.7 kB/s \n",
            "\u001b[?25hInstalling collected packages: nvidia-cublas-cu11, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, tensorrt\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 tensorrt-8.5.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check TensorRT version\n",
        "---"
      ],
      "metadata": {
        "id": "9i0KY7s4IXqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorrt as trt\n",
        "trt.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "15MnskZTIaVl",
        "outputId": "dc5be8ca-45b7-4ec1-9e09-816ecb79b344"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'8.5.2.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import requried libraries\n",
        "---"
      ],
      "metadata": {
        "id": "s3OINNECIksU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import argparse\n",
        "import tensorrt as trt\n",
        "import random\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pycuda.autoinit\n",
        "import pycuda.driver as cuda\n",
        "import tensorrt as trt\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "_EYH-wi_HDJt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define constants\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qigYtiWGJigE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "engine_path = 'model.trt'\n",
        "input_name = 'conv2d_input'\n",
        "Input_shape = (3,256,256)"
      ],
      "metadata": {
        "id": "ffW0wyjSJh5z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** Load the engine we created in previous steps."
      ],
      "metadata": {
        "id": "CI28CRzjGL5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
        "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
        "print(\"Load the created tensorrt engine\")\n",
        "with open(engine_path, 'rb') as f:\n",
        "    engine_data = f.read()\n",
        "engine = trt_runtime.deserialize_cuda_engine(engine_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOsp1s4kGLRs",
        "outputId": "84ebaf92-e4c0-4102-d61d-a18bfe28ead2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the created tensorrt engine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple helper data class that's a little nicer to use than a 2-tuple.\n",
        "class HostDeviceMem(object):\n",
        "    def __init__(self, host_mem, device_mem):\n",
        "        self.host = host_mem\n",
        "        self.device = device_mem\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__str__()"
      ],
      "metadata": {
        "id": "YPjdVn5CJA-r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZRJmEZlFE4_l"
      },
      "outputs": [],
      "source": [
        "# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\n",
        "def allocate_buffers(engine):\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "    bindings = []\n",
        "    stream = cuda.Stream()\n",
        "    for binding in engine:\n",
        "        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
        "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
        "        # Allocate host and device buffers\n",
        "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
        "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
        "        # Append the device buffer to device bindings.\n",
        "        bindings.append(int(device_mem))\n",
        "        # Append to the appropriate list.\n",
        "        if engine.binding_is_input(binding):\n",
        "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "        else:\n",
        "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
        "    return inputs, outputs, bindings, stream"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
        "context = engine.create_execution_context()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZQdLD4SHbsT",
        "outputId": "da9615cb-ecbd-4a5c-d033-43c38e746f1d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-4e2a1eedc286>:8: DeprecationWarning: Use get_tensor_shape instead.\n",
            "  size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
            "<ipython-input-9-4e2a1eedc286>:8: DeprecationWarning: Use network created with NetworkDefinitionCreationFlag::EXPLICIT_BATCH flag instead.\n",
            "  size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
            "<ipython-input-9-4e2a1eedc286>:9: DeprecationWarning: Use get_tensor_dtype instead.\n",
            "  dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
            "<ipython-input-9-4e2a1eedc286>:16: DeprecationWarning: Use get_tensor_mode instead.\n",
            "  if engine.binding_is_input(binding):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = np.float32(np.random.rand(Input_shape[0] ,Input_shape[1], Input_shape[2]))\n",
        "\n",
        "inputs[0].host = test_image"
      ],
      "metadata": {
        "id": "QVvg_s2sH2h8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_inference_v2(context, bindings, inputs, outputs, stream):\n",
        "    # Transfer input data to the GPU.\n",
        "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
        "    # # Run inference.\n",
        "    # context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
        "    # # Transfer predictions back from the GPU.\n",
        "    # [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
        "    # # Synchronize the stream\n",
        "    # stream.synchronize()\n",
        "    # # Return only the host outputs.\n",
        "    # return [out.host for out in outputs]"
      ],
      "metadata": {
        "id": "3S-kIJ-pJySx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trt_outputs = do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "4GN-6FrZJM7b",
        "outputId": "d33907b1-5ee8-4a45-9d00-81dd6563c068"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LogicError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLogicError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-97abf4f47c7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_inference_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbindings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbindings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-6058ec1482f3>\u001b[0m in \u001b[0;36mdo_inference_v2\u001b[0;34m(context, bindings, inputs, outputs, stream)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdo_inference_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbindings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Transfer input data to the GPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemcpy_htod_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# # Run inference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-6058ec1482f3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdo_inference_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbindings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Transfer input data to the GPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemcpy_htod_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# # Run inference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLogicError\u001b[0m: cuMemcpyHtoDAsync failed: invalid argument"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IAr7Hf6tJzRI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}